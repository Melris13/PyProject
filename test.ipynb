{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test récupération data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ... previous code ...\n",
    "\n",
    "\n",
    "def fetch_episode_data(url):\n",
    "    # Récupération de la page web\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # s'assure que la requête a réussi\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Récupérer le mois courant pour filtrer les épisodes\n",
    "    current_month = datetime.now().strftime(\"%B\").lower()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Find all the 'td' elements with the class 'floatleftmobile td_jour'\n",
    "    days = soup.find_all('td', class_='floatleftmobile td_jour')\n",
    "\n",
    "    for day in days:\n",
    "        class_regex = re.compile(\"div_jour(courant)?\")\n",
    "        # Find the date within the 'div' with class 'div_jour'\n",
    "        date_div = day.find('div', class_=class_regex)\n",
    "        if date_div:\n",
    "            date_text = date_div.get('id')\n",
    "\n",
    "            # Find all the series entries within the 'span' with class 'calendrier_episodes'\n",
    "            episodes = day.find_all('span', class_='calendrier_episodes')\n",
    "            for episode in episodes:\n",
    "                # Extracting information using the tags and structure provided\n",
    "                series_info = episode.find('a', style=True)\n",
    "                episode_info = episode.find('a', class_='liens')\n",
    "                country_img = episode.find_previous_sibling('img')\n",
    "                network_img = country_img.find_next_sibling('img')\n",
    "\n",
    "                name = series_info.get('title')\n",
    "                episode_detail = episode_info.get('alt')\n",
    "                origin_country = country_img.get(\n",
    "                    'alt') if country_img else None\n",
    "                network = network_img.get('alt') if network_img else None\n",
    "                episode_url = episode_info.get('href')\n",
    "\n",
    "                # Parse out the season and episode numbers from the text\n",
    "                season_episode_match = re.search(\n",
    "                    r'saison (\\d+) episode (\\d+)', episode_detail)\n",
    "                if season_episode_match:\n",
    "                    season_num = int(season_episode_match.group(1))\n",
    "                    episode_num = int(season_episode_match.group(2))\n",
    "\n",
    "                    # Add to the data list\n",
    "                    episode_data = {\n",
    "                        'name': name,\n",
    "                        'season_num': season_num,\n",
    "                        'episode_num': episode_num,\n",
    "                        # Format the date as required\n",
    "                        'date': date_text,\n",
    "                        'origin_country': origin_country,\n",
    "                        'network': network,\n",
    "                        'episode_url': episode_url\n",
    "                    }\n",
    "                    data.append(episode_data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# URL du site\n",
    "url = 'https://www.spin-off.fr/calendrier_des_series.html'\n",
    "episode_data = fetch_episode_data(url)\n",
    "\n",
    "# Afficher les données récupérées\n",
    "for episode in episode_data:\n",
    "    print(episode)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettre tous les éléments dans un fichier .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données ont été enregistrées dans 'data/files\\episodes.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Chemin vers le dossier où le fichier CSV sera enregistré\n",
    "folder_path = 'data/files'\n",
    "file_name = 'episodes.csv'\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "episode_data = fetch_episode_data(url)\n",
    "\n",
    "# Créer le dossier s'il n'existe pas déjà\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Enregistrement dans le fichier CSV\n",
    "with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file, delimiter=';')\n",
    "\n",
    "    # Écriture de l'en-tête du fichier CSV\n",
    "    writer.writerow(['nom_serie', 'numero_de_lepisode', 'numero_de_la_saison', 'date', 'pays_origine', 'reseau', 'url_episode'])\n",
    "\n",
    "    # Écriture des données des épisodes\n",
    "    for episode in episode_data:\n",
    "        writer.writerow([\n",
    "            episode['name'],\n",
    "            episode['episode_num'],\n",
    "            episode['season_num'],\n",
    "            episode['date'],\n",
    "            episode['origin_country'],\n",
    "            episode['network'],\n",
    "            episode['episode_url']\n",
    "        ])\n",
    "\n",
    "print(f\"Les données ont été enregistrées dans '{file_path}'\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqllite add to table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données ont été insérées dans la base de données 'data/databases\\database.db'\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Supposons que episodes_data est votre liste de dictionnaires contenant toutes les données d'épisode\n",
    "episode_data = fetch_episode_data(url)\n",
    "\n",
    "# Chemin vers le dossier où la base de données SQLite sera enregistrée\n",
    "db_folder_path = 'data/databases'\n",
    "# Créer le dossier s'il n'existe pas déjà\n",
    "os.makedirs(db_folder_path, exist_ok=True)\n",
    "\n",
    "# Chemin complet de la base de données SQLite\n",
    "db_file_path = os.path.join(db_folder_path, 'database.db')\n",
    "\n",
    "# Connection à la base de données SQLite\n",
    "conn = sqlite3.connect(db_file_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Créer la table `episode`\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS episode (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT NOT NULL,\n",
    "    season_num INTEGER NOT NULL,\n",
    "    episode_num INTEGER NOT NULL,\n",
    "    date TEXT,\n",
    "    origin_country TEXT,\n",
    "    network TEXT,\n",
    "    episode_url TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insérer les données dans la table `episode`\n",
    "for episode in episode_data:\n",
    "    cursor.execute('''\n",
    "    INSERT INTO episode (name, season_num, episode_num, date, origin_country, network, episode_url)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        episode['name'],\n",
    "        episode['season_num'],\n",
    "        episode['episode_num'],\n",
    "        episode['date'],\n",
    "        episode['origin_country'],\n",
    "        episode['network'],\n",
    "        episode['episode_url']\n",
    "    ))\n",
    "\n",
    "# Valider les changements\n",
    "conn.commit()\n",
    "\n",
    "# Fermer la connexion\n",
    "conn.close()\n",
    "\n",
    "print(f\"Les données ont été insérées dans la base de données '{db_file_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81bd72421fd5d63e4ee9fa784821c92c07d954c88ac6df7776d2c7cf9a96f666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
